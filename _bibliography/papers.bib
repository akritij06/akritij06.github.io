@inproceedings{jain2025doc2chart,
  abbr={EMNLP},
  title={Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents},
  author={Jain, Akriti and Ramu, Pritika and Garimella, Aparna and Saxena, Apoorv},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  year={2025},
  selected={true},
  arxiv={2507.14819},
  abstract={Large Language Models (LLMs) have demonstrated strong capabilities in transforming 
      text descriptions or tables to data visualizations via instruction-tuning methods. However, it 
      is not straightforward to apply these methods directly for a more real-world use case of 
      visualizing data from long documents based on user-given intents, as opposed to the user 
      pre-selecting the relevant content manually. We introduce the task of intent-based chart 
      generation from documents: given a user-specified intent and document(s), the goal is to 
      generate a chart adhering to the intent and grounded on the document(s) in a zero-shot setting. 
      We propose an unsupervised, two-staged framework in which an LLM first extracts relevant 
      information from the document(s) by decomposing the intent and iteratively validates and refines 
      this data. Next, a heuristic-guided module selects an appropriate chart type before final code 
      generation. To assess the data accuracy of the generated charts, we propose an attribution-based 
      metric that uses a structured textual representation of charts, instead of relying on visual 
      decoding metrics that often fail to capture the chart data effectively. To validate our 
      approach, we curate a dataset comprising of 1,242 <intent, document, charts> tuples from two 
      domains, finance and scientific, in contrast to the existing datasets that are largely limited 
      to parallel text descriptions/ tables and their corresponding charts. We compare our approach 
      with baselines using single-shot chart generation using LLMs and query-based retrieval methods; 
      our method outperforms by upto 9 points and 17 points in terms of chart data accuracy and chart 
      type respectively over the best baselines.}
}

@inproceedings{jain2025first,
  abbr={EMNLP},
  title={FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction},
  author={Jain, Akriti and Sharma, Saransh and Mukherjee, Koyel and Pal, Soumyabrata},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2025},
  year={2025},
  selected={true},
  arxiv={2410.12513},
  abstract={Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across 
      different domains such as vision and language processing. However, due to sequential processing 
      through a stack of transformer layers, autoregressive decoding faces significant computation/
      latency challenges, particularly in resource-constrained environments like mobile and edge 
      devices. Existing approaches in literature that aim to improve latency via skipping layers have 
      two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at 
      pre-determined layers irrespective of input sequence. Both the above strategies have limitations 
      - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework 
      and the latter does not capture the variation in layer importance across tasks or more 
      generally, across input sequences. To address both limitations, we propose FiRST, an algorithm 
      that reduces inference latency by using layer-specific routers to select a subset of transformer 
      layers adaptively for each input sequence - the prompt (during the prefill stage) decides which 
      layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling 
      faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on 
      any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different 
      task-specific middle layers play a crucial role in evolving hidden representations depending on 
      tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming 
      other layer selection strategies in quality metics. It retains competitive performance to base 
      model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a 
      promising and efficient solution for LLM deployment in low-resource environments.}
}

@article{jain2025contextual,
  title={Modeling Contextual Passage Utility for Multihop Question Answering},
  author={Akriti Jain and Aparna Garimella},
  journal={Under review at ACL ARR},
  year={2025},
  selected={true}
  abstract={Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multi-hop reasoning, that a passage's utility can be context-dependent, influenced by its relation to other passagesâ€”whether it provides complementary information, or forms a crucial link in conjunction with others. In this paper, we propose a light-weight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question, to obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to better reranking and downstream task performance compared to relevance-based reranking methods.}
}

@article{jain2025sufficiency,
  title={Knowing What's Missing: Assessing Information Sufficiency in Question Answering},
  author={Akriti Jain and Aparna Garimella},
  journal={Under review at ACL ARR},
  year={2025},
  selected={true},
  abstract={Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success for factual questions, including both single-hop and multi-hop queries, they frequently fail on inferential questions that require reasoning beyond direct text extraction. We hypothesize that for such cases, asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency than a direct judgment does. Specifically, we propose a structured, two-step Identify-then-Verify framework for robust sufficiency modeling. Our method first identifies what specific information appears to be missing from the provided context by generating and finding a consensus among multiple hypotheses. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this supposedly missing information is truly absent. We evaluate our method against established baselines across a diverse suite of datasets, including multi-hop and factual QA. The results demonstrate that by forcing the model to justify its claims about missing information, our framework produces a more accurate sufficiency judgment while detailing any information gaps.}
}

@misc{park2025mirage,
  abbr={ICLR},
  title={MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions},
  author={Park, Jeonghyun and Baek, Ingeol and Yoon, Seunghyun and Jang, Haeun and Garimella, Aparna and Jain, Akriti and Lipka, Nedim and Lee, Hwanhee},
  year={2025},
  eprint={2509.22750},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/2509.22750},
  abstract={Real-world Multi-hop Question Answering (QA) often involves ambiguity that is 
      inseparable from the reasoning process itself. This ambiguity creates a distinct challenge, 
      where multiple reasoning paths emerge from a single question, each requiring independent 
      resolution. Since each sub-question is ambiguous, the model must resolve ambiguity at every 
      step. Thus, answering a single question requires handling multiple layers of ambiguity 
      throughout the reasoning chain. We find that current Large Language Models (LLMs) struggle in 
      this setting, typically exploring wrong reasoning paths and producing incomplete answers. To 
      facilitate research on multi-hop ambiguity, we introduce MultI-hop Reasoning with AmbiGuity 
      Evaluation for Illusory Questions (MIRAGE), a benchmark designed to analyze and evaluate this 
      challenging intersection of ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,
      142 high-quality examples of ambiguous multi-hop questions, categorized under a taxonomy of 
      syntactic, general, and semantic ambiguity, and curated through a rigorous multi-LLM 
      verification pipeline. Our experiments reveal that even state-of-the-art models struggle on 
      MIRAGE, confirming that resolving ambiguity combined with multi-step inference is a distinct and 
      significant challenge. To establish a robust baseline, we propose CLarifying Ambiguity with a 
      Reasoning and InstructiON (CLARION), a multi-agent framework that significantly outperforms 
      existing approaches on MIRAGE, paving the way for more adaptive and robust reasoning systems.}
}
